<!--
date: 2025-01-16T22:54:36
-->


[https://youtu.be/AtuB7p-JU8Y](https://youtu.be/AtuB7p-JU8Y)

![YouTube Preview](https://img.youtube.com/vi/AtuB7p-JU8Y/mqdefault.jpg)



The video compares the work of **Cursor**  (proprietary fork of VS Code) and **Cline**  (open-source VS Code extension) - they edit a React project of 240k code tokens.

Here both use claude-3.5-sonnet.

Cursor also uses some embedding model ([either with OpenAIâ€™s embedding API or by a custom embedding model](https://forum.cursor.com/t/codebase-indexing/36) ) and a cloud vector database to vectorize code chunks for semantic search. [[Explanation with pictures](https://medium.com/@wangxj03/semantic-code-search-010c22e7d267) ]

The first simple task was completed by both in 1 minute, but Cline returned broken code, on task 3 it "looped".

That is, Cursor Composer won 3 out of 3.