<!--
date: 2024-04-18T23:45:16
-->

ðŸ¥³ Llama 3 is out!

Models were trained on two newly announced clusters with specially built 24K GPUs on over 15T tokens of data - the training dataset is 7 times larger than that used for Llama 2,
including **4 times more code**.

[https://llama.meta.com/llama3/](https://llama.meta.com/llama3/) 

models are also deployed on LPU [https://groq.com/](https://groq.com/)