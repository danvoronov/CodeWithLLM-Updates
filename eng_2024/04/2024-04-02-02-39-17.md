<!--
date: 2024-04-02T02:39:17
-->

Chatbot Arena [and](https://chat.lmsys.org/?arena)Perplexity playground [, the](https://labs.perplexity.ai/)dbrx-instruct**model (**github [) has appeared. I conducted a series of tests with code generation, and indeed, the results are worthy. Moreover, it's](https://github.com/databricks/dbrx)faster than CodeLLaMA**-70B.**The developer of the VSCode plugin_Double_ [added GPT-4 Turbo and Claude 3 (Opus) to DBRX Instruct, although it's not very clear why, and GPT-5 is also on the waiting list.](https://docs.double.bot/models)_DataBricks, a company known for its data processing and analysis solutions, has released one of the most powerful and efficient_open**LLM - DBRX. On the graphs published in the**post with the presentation [of the model, DBRX surpasses other open solutions in the fields of mathematics and](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)programming**.

This**MoE 16x12B**model**A multi-expert model (132 billion general parameters - 36 billion active parameters for processing each token), which outperforms the open Grok-1 and closed GPT-3.5 Turbo (but not Claude 3 Haiku) in many tasks. The context window is 32k, and the tokenizer is the same as in GPT-4. Knowledge cutoff - December 2023.

They claim to outperform CodeLLaMA-70B in tests. The DBRX model is quite large, so not everyone can run it, but it's not as enormous as Grok-1, which almost no one can deploy now. Meta plans to release Lllama 3 sometime in July.

The chat is still on [https://huggingface.co/spaces/databricks/dbrx-instruct](https://huggingface.co/spaces/databricks/dbrx-instruct)(5-shoot max)