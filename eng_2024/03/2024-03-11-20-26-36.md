<!--
date: 2024-03-11T20:26:36
-->

During [a conversation with Greg Brockman](https://www.youtube.com/watch?v=CvgfxH0UZa4)

![YouTube Preview](https://img.youtube.com/vi/CvgfxH0UZa4/mqdefault.jpg)

(from OpenAI), we learn about CodeX - then a new edition of a large language model, focused on code generation. This happened on August 12, 2021, at an early stage of using such models for programming, so the conversation has**historical**value today. 

CodeX - the successor to GPT-3, but with numerous improvements for better understanding and generation of code. The model was trained on all text and open code on the Internet and can generate executable code based on natural language prompts.

Greg emphasizes the importance of ensuring high-quality input data and values during model training to prevent bias and undesirable behavior. He also sees the potential of CodeX in programming education, as the model can provide explanations and guidance in the form of code. At the same time, there are copyright and access issues that need to be resolved when deploying such systems.