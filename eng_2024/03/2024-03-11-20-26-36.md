<!--
date: 2024-03-11T20:26:36
-->

During the [conversation with Greg Brockman](https://www.youtube.com/watch?v=CvgfxH0UZa4) 

![YouTube Preview](https://img.youtube.com/vi/CvgfxH0UZa4/mqdefault.jpg)

 (from OpenAI), we learn about CodeX - then a new version of a large language model focused on code generation. This took place on August 12, 2021, at an early stage in the use of such models for programming, so the conversation today has **historical**  value.

CodeX is a descendant of GPT-3, but with numerous improvements for better understanding and code generation. The model is trained on all text and open source code on the Internet and can generate executable code based on natural language prompts.

Greg emphasizes the importance of ensuring high quality of input data and values during model training to prevent bias and unwanted behavior. He also sees the potential of CodeX in programming education, as the model can provide explanations and guidance in the form of code. At the same time, there are copyright and access issues that need to be addressed when deploying such systems.