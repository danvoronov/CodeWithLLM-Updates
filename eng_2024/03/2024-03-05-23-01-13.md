<!--
date: 2024-03-05T23:01:13
photo: ![Photo](2024-03-05-23-01-13.jpg)


-->

![Photo](2024-03-05-23-01-13.jpg)

I want to share my thoughts about LLM leaderboards. The thing is that**exactly**testing and evaluating models is quite difficult. Performance can vary depending on the type of task and context.

I think it doesn't make sense to delve too deeply into a detailed comparison of model positions in leaderboards. Instead, it's better to divide them into several groups: leaders, middle, and outsiders. This will give a more realistic idea of their capabilities and help avoid excessive focus on minor differences in scores.

ðŸ¤—_On_ [bigcode-models-leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)_, only open models are available, on the screen, I filtered out instruct with which you can interact like in a chat, providing instructions._In general, DeepSeek and Phind-CodeLlama of sizes 33B and 34B demonstrated the best performance. The table does not yet have [Phind-CodeLlama 70B](https://www.phind.com/plans), and it is still unknown whether its developers will release it as open-source