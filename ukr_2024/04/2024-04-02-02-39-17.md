<!--
date: 2024-04-02T02:39:17
-->

У розділі чатів на  [Chatbot Arena](https://chat.lmsys.org/?arena) та у  [Perplexity playground](https://labs.perplexity.ai/) з'явилася **dbrx-instruct** модель ( [github](https://github.com/databricks/dbrx)). Я провів низку тестів із генерації коду, і справді результати гідні. До того ж **швидче ніж CodeLLaMA**-70B.

_Розробник VSСode плагіну _ [Double](https://docs.double.bot/models)_ додав до GPT-4 Turbo та Claude 3 (Opus) теж DBRX Instruct, хоча не дуже зрозуміло навіщо та ще й GPT-5 вейтліст відкрив._

Компанія DataBricks, відома своїми рішеннями для обробки та аналізу даних,  випустила одну з найпотужніших та найефективніших **відкритих** LLM - DBRX.  На графіках, які опубліковані  [в пості з презентацією](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) моделі, DBRX випереджає інші відкриті рішення в галузях математики та **програмування**.

Ця **MoE 16x12B** мультиекспертна модель (132 мільярди загальних параметрів - 36 мільярдів активних параметрів для обробки кожного токену), яка у багатьох завданнях перевершуює відкриту Grok-1 та закриту GPT-3.5 Turbo (але не Claude 3 Haiku). Контекстне вікно 32k, токенайзер як й у GPT-4. Knowledge cutoff - грудень 2023.

Вони говорять, що за тестами перевершують CodeLLaMA-70B. Модель DBRX досить великого розміру, щоб не кожен міг її запустити, проте не настільки величезна, як Grok-1, яку зараз практично ніхто не зможе розгорнути у себе. Meta планує випустити Lllama 3 десь у липні.

Чат ще є на  [https://huggingface.co/spaces/databricks/dbrx-instruct](https://huggingface.co/spaces/databricks/dbrx-instruct)
(5-shoot max)